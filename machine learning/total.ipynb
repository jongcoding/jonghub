{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1주차 기계학습이란\n",
    "\n",
    "\n",
    "기계학습 소개\n",
    "    인공지능, 머신러닝, 딥러닝의 개념\n",
    "        (1) 인공지능: 사고나 학습등 인간이 가진 지적 능력을 컴퓨터를 통해 구현하는 기술\n",
    "        (2) 머신러닝: 컴퓨터가 스스로 학습하여 인공지능의 성능을 향상시키는 기술 방법\n",
    "        (3) 딥러닝: 인간이 뉴러과 비슷한 인공신경망 방식으로 정보를 처리\n",
    "\n",
    "    기계학습의 정의와 중요성\n",
    "        - 학습이란? <표준국어대사전>: \"겸헝의 결과로 나타나는, 비교적 지속적인 행동의 변화나 그 잠재력의 변화, 또는 지식을 습득하는 과정\n",
    "        - 기계학습이란?: 인공지능 초창기 사무엘의 정리\n",
    "            \" 컴퓨터가 경험을 통해 학습할 수 있도록 프로그래밍할 수 있다면, 세세하게 프로그래밍해야 하는 번거로움에 벗어날 수 있다.\n",
    "        - 기계 학습이란? - 현대적 정의\n",
    "            \"어떤 컴퓨터 프로그램이 T라는 작업을 수행한다. 이 프로그램의 성능을 P라는 척도로 평가했을 때 경험E를 통해 성능이 개선된다면 이 프로그램은 학습을 한다고 말할 수 있다.\n",
    "            \" 사례 데이터, 즉 과거 경험을 이용하여 성능 기준을 최적화하도록 프로그래밍하는 작업\n",
    "            \" 성능을 개선하거나 정확하게 예측하기 위해 경험을 이용하는 계산학 방법들 \n",
    "        - 자식기반 방식에서 기계 학습으로의 대전환 \n",
    "            - 인공지능의 탄생\n",
    "                - 컴퓨터의 뛰어난 능력\n",
    "                    - 사람이 어려워하는 일을 아주 쉽게함\n",
    "                    - 80932.46789076*0.39001324와 같은 곱셈을 고속으로 수행\n",
    "                    - 복잡한 함수의 미분과 적분 척척\n",
    "                - 컴퓨터에 대한 기대감(컴퓨타의 능력 과신)\n",
    "                    - 사람이 쉽게하는 일, 예를들어 고양이/개 구별하는 일도 잘 하지 않을까\n",
    "                    - 1950년대에 인공지능이라는 분야 등장\n",
    "            \n",
    "            - 초창기는 지식기반 방식이 주류\n",
    "                - ex: \"구멍이 2개이고 중간 부분이 홀쭉하며, 맨 위와 아래가 둥근 모양이라면 8이다.\"\n",
    "            - 큰 꺠달음\n",
    "                - 지식기반의 한계\n",
    "                - 단추를 \"가운데 구멍이 몇 개 있는 물체\"라고 규정하면 많은 오류 발생\"\n",
    "                - 사람은 변화가 심한 장면을 아주 쉽게 인식하지만, 왜 그렇게 인식하는지 서술하지는 못함\n",
    "            - 인공지능의 주도권 전환\n",
    "                - 지식기반-> 기계학습\n",
    "                - 기계학습: 데이터 중심 접근방식\n",
    "\n",
    "        기계학습 개념\n",
    "            - 간단한 기계학습 예제\n",
    "                - 가로축은 시간, 세로축은 이동체의 위치\n",
    "                - 관측한 4개의 점이 데이터\n",
    "            - 예측 문제\n",
    "                - 임의의 시간이 주어지면 이떄 이동체의 위치는?\n",
    "                - 회귀문제와 분류 문제로 나뉨\n",
    "                    - 회귀는 목표치가 실수, 분류는 부류값\n",
    "            - 훈련집합\n",
    "                - 가로축은 특징, 세로축은 목표치\n",
    "                - 관측한 4개의 점이 훈련집합을 구서함\n",
    "                훈련집합 x={x1,x2,..., Xn}, Y={y1,y2,..,Yn}\n",
    "            - 데이터를 어떻게 모델링할 것인가\n",
    "                -눈대중으로 보면 직석을 이루므로 직선을 선택하자-> 모델로 직선을 선택한 셈\n",
    "            - 직선 모델의 수식\n",
    "                - 2개의 매개변수w와 b\n",
    "                y=wx+b\n",
    "            - 기계학습은\n",
    "                - 가장 정확하게 예측할 수 있는. 즉 최적의 매개변수를 찾는 작업\n",
    "                - 처음에는 최적값을 모르므로 임의의 값에서 시작하고. 점점 성능을 개선하여 최적에 도달\n",
    "            - 학습에 마치면\n",
    "                - 예측에 사용\n",
    "                - 10.0순간의 이동체 위치를 알고자하면, f3(10.0)=0.5*10.0+2.0=7.0이라고 예측함\n",
    "            - 기계학습의 궁극적인 목표\n",
    "                - 훈련집합에 없는 새로운 샘플에 대한 오류를 최소화(새로운 샘플집합: 테스트 집합)\n",
    "                - 테스트 집합에 대한 높은 성능을 일반화 능력이라고 부름\n",
    "            - 사람의 학습과 기계학습\n",
    "-> 추가해야함                \n",
    "    특징 공간과 데이터에 대한 이해\n",
    "        - 1차원과 2차원 특징 공간\n",
    "            - 1차원 특징공간\n",
    "            - 2차원 특징공간\n",
    "                -특징 벡터 표기\n",
    "                    - x=(x1,x2)T\n",
    "                - EX\n",
    "                    - x=(몸무게 키)T, y=장타율\n",
    "                    - x=(체온, 두통)T, y=감기여부\n",
    "\n",
    "        - 다차원 특징 공간\n",
    "            - 다차원 특징공간예제\n",
    "            - d-차원데이터\n",
    "                - 특징벡터 표기 x=(x1,x2,...,xd)T\n",
    "            - d- 차원데이터를 위한 학습 모델\n",
    "                - 직선 모델을 사용하는 경우 매개변수 수= d+1\n",
    "                    y=w1x1+w2x2+...+wdxd+b\n",
    "                - 2차 곡선모델을 사용하면 매개변수 수가 크게 증가\n",
    "                    - 매개변수 수= d^2+d+1\n",
    "                    - ex: lris 데이터: d=4이므로 21개의 매개변수\n",
    "                    - ex: MNIST 데이터: D=784이므로 615,441개의 매개변수\n",
    "                    y=w1x^2+ w2x2/2+...wdxd^2+ wd+1x1x2+...wd2xd-1xd+wd^2+1x1+...+Wd^2+dxd+b\n",
    "        - 데이터에 대한 이해\n",
    "            -> 이해\n",
    "                - 과학 기술의 발전과정\n",
    "                    데이터 수집-> 모델 정립-> 예측\n",
    "                - 기계 학습\n",
    "                    -기계 학습이 푸는 문제는 훨씬 복잡함\n",
    "                        ex- '8' 숫자 패턴과 '단추' 패턴의 다양한 변화 양상\n",
    "                    - 단순한 수학 공식으로 표현 불가능\n",
    "                    - 자동을 모델을 찾아내는 과정이 필수\n",
    "            - 데이터 생성과정\n",
    "                - 데이터 생성 과정을 완전히 아는 인위적 상황의 예제\n",
    "                    ex- 두 개 주사위를 던져 나온 눈의 합을 x라 할 때, y=(x-7)^2+1점을 받는 게임\n",
    "                    - 이런 상황을 '데이터 생성 과정을 완전히 알고 있다'고 말함\n",
    "                        - x를 알면 정확히 y를 예측할 수 있음\n",
    "                            - 실제 주사위를 던져 x={3,10,8.5}를 얻었다면,y={17,10,2.5}\n",
    "                        -x의 발생 확률 P(x)를 정확히 알 수 있음\n",
    "                        - P(x)를 알고 있으므로, 새로운 데이터 생성 가능\n",
    "                - 실제 기계 학습문제\n",
    "                    - 데이터 생성 광정을 알 수 없음\n",
    "                    - 단지 주어진 훈련집합 X,Y로 예측 모델 또는 생성 모델을 근사 추정할 수 있을 뿐 \n",
    "\n",
    "            - 데이터 베이스의 중요성\n",
    "                - 데이터베이스의 품질\n",
    "                    - 주어진 응용에 맞느 충분히 다양한 데이터를 충분한 양만큼 수집-> 추정 정확도 높아짐\n",
    "                        - 예: 정면 얼굴만 가진 데이터베이스로 학습하고 나면, 기운 얼굴은 매운 낮은 성능\n",
    "                        -> 주어진 응용 환경을 자세히 살핀 다음 그에 맞는 데이터 베이스 확보는 아주 중요함\n",
    "                - 아주 많은 공개 데이터베이스\n",
    "                    - 기계 학습의 초파리로 여겨지는 3가지 데이터베이스:Iris, MNIST, ImageNet\n",
    "                    - 위키피디아에서 'list of datasets for machine learning research'로 검색\n",
    "                    - UCI Repository(현재 기준으로 622개 데이터베이스 제공)\n",
    "\n",
    "            - 데이터베이스 크기와 기계학습 성능\n",
    "                - 데이터베이스의 왜소한 크기\n",
    "                    -ex: MNIST: 28*28 흑백 비트맵이라면 서로 다른 총 샘플 수는 2^784가지이지만, MNIST는 고작 6만 개 샘플\n",
    "                - 왜소한 데이터 베이스로 어떻게 높은 성능을 달성하는가?\n",
    "                    - 방대한 공간에서 실제 데이터가 발생하는 곳은 매우 작은 부분 공간임\n",
    "                    - 매니 폴드 가정\n",
    "                        - 2222 와같이 일정한 규칙에 따라 매끄럽게 변화\n",
    "\n",
    "            - 데이터 가시화 \n",
    "                - 4차원 이상의 초공간은 한꺼번에 가시화 불가능\n",
    "                - 여러 가지 가시화 기법\n",
    "                    - 2개씩 조합하여 여러 개의 그래프 그림\n",
    "\n",
    "    기계학습의 예와 모델 선택\n",
    "        - 과소적합과 과잉 적합\n",
    "            - 아래 그름의 1차 모델은 과소적합\n",
    "                - 모델의 '용량이 작아' 오차가 클 수 밖에 없는 현상\n",
    "            - 비선형 모델을 사용하는 대안\n",
    "                - 2차, 3차, 4차, 12차는 다항식 곡선을 선택한 예\n",
    "                - 1차(선형)에 비해 오차가 크게 감소함\n",
    "\n",
    "            - 과잉 적합\n",
    "                - 12차 다항식 곡선을 채택한다면 훈련집합에 대해 거의 완벽하게 근사화함\n",
    "                - 하지만 '새로운' 데이터를 예측해야 하지만 빨간점을 예측 \n",
    "                - 이유는 '용량이 크기'때문. 학습과정에서 잡음까지 수용-> 과잉적합 현상\n",
    "                - 적절한 용얄의 모델을 선택하는 모델 선택 작업이 필요함\n",
    "\n",
    "        - 바이어스와 분산\n",
    "            - 1차~12차 다항식 모델의 비교관찰\n",
    "                - 1~2차는 훈련집합과 테스트집합 모두 낮은 성능\n",
    "                - 12차는 훈련집합에 높은 성능을 보이나 테스트집합에는 낮은 성능-> 낮은 일반화 능력\n",
    "                - 3~4차는 훈련집합에 대해 12차보다는 낮겠지만 테스트 집합에는 높은 성능\n",
    "                    -> 높은 일반화 능력\n",
    "            \n",
    "            - 훈련집합을 여러 번 수집하여 1차~12차에 적용하는 실험\n",
    "                - 2차는 매번 큰 오차-> 바이어스가 큼. 하지만 비슷한 모델을 얻음 -> 낮은 분산\n",
    "                - 12차는 매번 작은 오차-> 바이어스가 작음. 하지만 크게 다른 모델을 얻음-> 높은 분산\n",
    "                - 일반적으로 용량이 적은 모델은 바이어스는 크고 분산은 작음. 복잡한 모델은 바이어스는 작고 분산은 큼\n",
    "                - 바이어스와 분산은 트레이드오프 관계\n",
    "            \n",
    "            - 기계학습의 목표\n",
    "                - 낮은 바이어스와 낮은 분산을 가진 예측기 제작이 목표, 즉 왼쪽 아래 상황\n",
    "                    - 하지만 바이어스와 분산은 트레이드오프관계\n",
    "                    - 따라서 바이어스 희생을 최소로 유지하며 분산을 최대로 낮추는 전략이 필요\n",
    "                    \n",
    "        - 모델 선택의 한계와 현실적인 해결책 \n",
    "            - 알고리즘에서 모델 집합 옴\n",
    "                - 서로 다른 차수의 다항식이 옴(1차,2차,3차,...,12차)\n",
    "                - 현실에서는 아주 다양\n",
    "                    -Support Vector Machine(SVM), 트리분류기, 신경망, 강화 학습 등이 선택 대상\n",
    "            - 현실에서는 경험으로 큰 틀 먼저 선택\n",
    "                -모델 선택 알고리즘으로 세부 모델 선택하는 전략 사용\n",
    "            - 이런 경험적인 접근 방법에 대한 'DEEP LEARNING'책의 비유\n",
    "                \"어느 정도 우리가 하는 일은 항상 둥근 홈(우리가 선택한 모델)에 네모 막대기(데이터 생성 과정)을 끼워 넣는 것이라고 말할 수 있다\"\n",
    "            \n",
    "            현대 기계학습의 전략\n",
    "                - 용량이 충분히 큰 모델을 선택한 후, 선택한 모델이 정상을 벗어나지 않도록 여러가제 규제 기번을 적용함\n",
    "                - EX: 12차 다항식을 선택한 후 적절히 규제 적용\n",
    "\n",
    "    규제의 개념\n",
    "        - 데이터 확대\n",
    "            - 데이터를 더 많이 수집하면 일반화 능력이 향상됨\n",
    "            - 데이터 수집은 많은 비용이 듬\n",
    "                - 그라운드 트루스를 사람이 일일이 레이블링해야 함\n",
    "            - 인위적으로 데이터 확대\n",
    "                - 훈련집합에 있는 샘프을 변형함\n",
    "                - 약간 회전 또는 와핑(부류 소속이 변하지 않게 주의)\n",
    "\n",
    "        - 가중치 감쇠\n",
    "            - 가중치를 작게 조잘하는 기법\n",
    "                - 아래 12차 곡선은 가중치가 매우 큼\n",
    "                    y=1005.7x^12-27774.4x^11+...-22852612.5x^1-12.8\n",
    "            - 가중치 감쇠는 개선된 목적함수를 이용하여 가중치를 작게 조절하는 규제 기법\n",
    "                - 식(1.11)의 두 번째 항으로서 가중치 크기를 작게 유지해줌\n",
    "-> 추가해야함\n",
    "\n",
    "기계학습 유형 \n",
    "    - 지도 방식에 따른 유형\n",
    "        - 지도 학습(Supervised Learning)\n",
    "            - 정답이 주어진 상태에서 학습하는 알고리즘\n",
    "            - 훈련데이터로부터 하나의 함수를 유추\n",
    "            - 지도학습 종류\n",
    "                - 회귀분석(Regression)/예측: 연속적인 값을 출력하는 것\n",
    "                - 분류(Classifiaciton): 주어진 입력 벡터가 어떤 종류의 값인지 표시하는 것\n",
    "        - 비지도 학습(Unsupervised Learning)\n",
    "            - 정답이 주어지지 않은 상태에서 데이터의 특정을 학습하는 알고리즘\n",
    "            - 데이터가 어떻게 구성되었는지를 알아내는 문제를 해결\n",
    "            - 지도학습혹은 강화 학습과는 달리 입력값에 대한 목표치(분류된 값)기 없음\n",
    "            - 비지도학습 종류: 군집화, 연관규칙\n",
    "                -데이터의 주요 특징을 요약하고 설명\n",
    "        - 강화학습(Reinforcement Learning)\n",
    "            - 보상 혹은 벌칙과 함께 여러 번의 시행착오를 거쳐 스스로 학습하는 방법\n",
    "                - 분류할 수 있는 데이터 혹은 정답이 존재하지 않음\n",
    "                - 행동에 보상 혹은 벌칙을 받으며 학습\n",
    "            - 보상을 최대한 많이 얻도록 하는 행동을 유도하도록 학습을 진행\n",
    "            - 예제: 아이가 일어서고 걷는 방법, 구글의 알파고 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2주차: 기계학습과 Python, 수학\n",
    "- 기계학습과 Python\n",
    "    * 파이썬은(python)은 데이터 과학 분야를 위한 표준 프로그래밍 언어\n",
    "        - 범용 프로그르매이 언어의 장점+ MATLAB과 R같은 특정 분야를 위한 스크립트 언어의 편리함\n",
    "        - 다양한 라이브러리\n",
    "            - 데이터 적재, 시각화, 통계, 자연어 처리, 이미지 처리 등에 필요한 라이브러리 존재\n",
    "        - 터미널이나 주피터 노트북 같은 도구로 대화하듯 프로그래밍\n",
    "        - 머신러닝과 데이터 분석은 데이터 주도 분석이라는 점에서 근본적으로 반복 작업, 따라서 반복 작업을 빠르게 처리하고 손쉽게 조작할 수 있는 도구가 필수\n",
    "        - 범용 프로그래밍 언어로서 파이썬은 복잡한 그래픽 사용자 인터페이스(GUI)나 웹 서비스도 만들 수 있으며 기존 시스템과 통합하기도 좋음 \n",
    "\n",
    "    - 라이브러리 및 도구들\n",
    "        * 오픈 소스인 사이킷런(scikit-learn)\n",
    "            - 다양한 머신러닝 알고리즘 + 알고리즘을 설명한 풍부한 문서제공\n",
    "            - 매우 인기가 높고 독보적인 파이썬 머신러닝 라이브러리\n",
    "            - 산업 현장이나 학계에도 널리 사용되고 많은 튜토리얼과 예전 코드\n",
    "            - 사이킷런은 다른 파이썬의 과학 패키지들과도 잘 연동\n",
    "        * 사이킷런 설치 \n",
    "            - scikit-learn은 두 개의 다른 파이썬 패키지인 넘파이와 사이파이를 사용\n",
    "            - 그래프를 그리려면 맷플롯립을, 대확식으로 개발할려면 아이파이썬(Ipython)과 주피터 노트북도 설치해야 함\n",
    "            - 필요한 패키지들을 모아 노흔 파이썬 배포판을 설치하는 방법을 권장\n",
    "                - Anaconda: 대용량 데이터 처리, 예측분석, 과학 계산용 파이썬 배포판 \n",
    "        - 주피터 노트북\n",
    "            - 주피터 노트북은 프로그램 코드를 브라우저에서 실행해주는 대화식 환경을 제공\n",
    "        - Numpy\n",
    "            - 파이썬으로 과학 계산을 하려면 꼭 필요한 패키지임. 다차원 배열을 위한 기능과 선형 대수 연산과 푸리에 변환 같은 고수준 수학 함수와 유사(pseudo) 난수 생성기를 포함\n",
    "        - SciPy\n",
    "            - 과학 계산용 함수를 모아놓은 파이썬 패키지임. SciPy는 고성능 선형 대수, 함수최적화, 신호처리, 특수한 수학 함수와 통계 분포 등을 포함한 많은 기능을 제공\n",
    "        - matplotlib\n",
    "            - 파이썬의 대표적인 과학 계산용 그래프 라이브러리임. 선그래프. 히스토그램, 산점도 등을 지원하며 출판에 쓸 수 있을 만큼의 고품질 그래프를 그려줌\n",
    "        - pandas\n",
    "            -데이터 처리와 분석을 위한 파이썬 라이브러리임\n",
    "\n",
    "    - 구글 코랩(Colab)\n",
    "        - Google Colab(colaboratory)\n",
    "            - 브라우저에서 python을 작성하고 실행가능\n",
    "            - 클라우드 기반으로 주피터 노트북 개발환경\n",
    "        - Colab 특징\n",
    "            - 별도의 파이썬 설치가 필요없음\n",
    "            - 데이터 분석 사용되는 라이브러리가 기본적으로 설치\n",
    "        - GPU를 무료로 사용 가능\n",
    "        - Jupyter 노트북과 비슷하지만 더 좋은 기능을 제공\n",
    "        -깃과 연동이 가능하여 사름들과 협업하여 코딩이 가능 \n",
    "\n",
    "- 기계학습과 수학 \n",
    "    * 기계학습에서 수학의 역할 \n",
    "        - 수학은 이론적인 배경과 알고리즘을 구성하는 기본적인 근간\n",
    "        - 알고리즘의 원리르 이해하기 위해 필요\n",
    "        - 사람은 알고리즘을 설계하고 데이터를 수집\n",
    "        - 기계학습은 수학/알고리즘/사람이 수집하는 데이터로 이루어진다.\n",
    "    * 선현대수학: 어떻게 조사 대상을 형식화 할 것인가?\n",
    "        - 학습 모델의 매개변수집합, 데이터 선형연산의 결합 등을 벡터 또는 행렬로 간결하게 표현 가능\n",
    "        - 데이터를 분석하여 유용한 정보를 알아내거나 특징 공간을 변환 수행\n",
    "    * 확률과 통계: 데이터의 특징을 어떻게 알 수 있을까?\n",
    "        - 데이터에 포함된 불확실성을 표현하고 처리\n",
    "        - 데이터의 특징들을 추출하고 많은 데이터의 중요한 속성만 간추리는 역할\n",
    "        - 모델 설계에 활용\n",
    "    * 최적화 이론: 알고리즘을 어떻게 이햏고, 훈련과정을 최적화 할 수 있을까?\n",
    "        - 목적함수를 최소화하는 최적해를 찾는데 활용(주로 미분을 활용)\n",
    "\n",
    "    -선형 대수\n",
    "        - 벡터와 행렬\n",
    "            * 백터\n",
    "                - 샘픔을 특징 벡터로 표현\n",
    "                - ex: Iris 데이터에 꽃받침의 길이, 꽃받침의 너비, 꽃잎의 길이, 꽃잎의 너비라는 4개의 특징이 각각 5.1, 3.5 1.4, 0.2인 샘플\n",
    "                - 여러 개의 특징 벡터를 첨자로 구분\n",
    "            * 행렬\n",
    "                - 여러 개의 벡터를 담음\n",
    "                - 훈련집합을 담은 행렬을 설계행렬이라 부름\n",
    "                - ex: Iris 데이터에 있는 150개의 샘프을 설계 행렬 X로 표현\n",
    "            * 행렬 A의 전치행렬 AT\n",
    "                A=341 라면 AT=3 0\n",
    "                  052         4 5\n",
    "                              1 2\n",
    "                - Iris의 설계 행렬을 전치행렬 표기에 따라 표현하면,\n",
    "                X=(XT1\n",
    "                   XT2\n",
    "                   XT150) \n",
    "            * 행렬을 이용하면 수학을 간결하게 표현할 수 있음\n",
    "                - 예: 다항식의 행렬 표현\n",
    "                f(x)=f(x1,x2,x3)=xTAx+bT+c\n",
    "            * 특수한 행렬들\n",
    "                정사각행렬, 대각행렬, 단위 행렬, 대칭행렬\n",
    "            * 행렬 연산\n",
    "                - 행렬 곱셈 C=AB, 즉 cij=인테그랄(k=1,s)aik*bkj\n",
    "                    * 교환법칙 성립하지않음: AB not BA\n",
    "                    * 분배법치과 결합법칙 성립: A(B+C)=AB+AC이고 A(BC)=(AB)C\n",
    "                - 벡터의 내적\n",
    "                    벡터의 내적 a*b=aTb=인테그랄(k=1,d)akbk\n",
    "                - 텐서\n",
    "                    - 3차원 이상의 구조를 가진 숫자 배열\n",
    "                    - 예: 3차원구조의 RGB컬러 영상 \n",
    "        - 놈과 유사도\n",
    "            * 벡터와 행렬의 크기를 놈으로 측정\n",
    "                - 벡터의 P차놈\n",
    "                    p=1: 벡터의 성분들의 절대값의 합\n",
    "                    p=2: 이는 윌가 일반적으로 벡터의 '길이'나 '크기'라고 부르는 것\n",
    "                    p=inf: 이는 벡터의 성분들 중 절대값이 가장 큰 것 \n",
    "                - 행렬의 프로베니우스 놈 p=2로\n",
    "            * 유사도와 거리\n",
    "                -벡터를 기학학적으로 해석\n",
    "                - 코사인 유사도\n",
    "                    cosine_similarity(a,b)=a/||a||*b/||b||b=cos(seta)\n",
    "        - 퍼셉트론의 해석\n",
    "            * 퍼셉트론\n",
    "                - 1958년 로젠블렛이 고안한 분류기 모델\n",
    "                - 퍼셉트론의 동작을 수식으로 표현하면,\n",
    "                o=taur(w*r), 이떄 taur(a)={1, a>=T}\n",
    "                                           -1. a<(T)\n",
    "                * 활성 함수 taur로는 계단함수 사용\n",
    "                - 파란직선은 두 개의 부분공간을 나누는 결정직선\n",
    "                    * W에 수직이고 원점으로부터 T/||W||2만큼 떨어져 있음\n",
    "                - 3차원 특징공간은 결정평면, 4차원 이상은 결정 초평면\n",
    "                - 예: 3차원 특징공간을 위한 퍼셉트론\n",
    "            * 출력이 여러 개인 퍼셉트론\n",
    "                출력은 벡타 O=(O1,O2,...,Oc)T로 표기\n",
    "                j번쨰 퍼셉트론의 가중치를 벡터를 wj=(wj1,wj2,...,wjd)T와 같이 표기\n",
    "                - 동작을 수식으로 표현하면, 행렬로 간결하게 쓰면 o=taur(Wx)\n",
    "                - 가중치 벡터를 각 부류의 기준 벡터로 간주하면, c개 부류의 유사도를 계산하는 셈\n",
    "            * 학습의 정리\n",
    "                - 아래 식은 학습을 마친 프로그램을 현장에 설치했을 때 일어나는 과정\n",
    "                    분류라는 과업: o=taur(Wx)\n",
    "                - 학습과정: 학습은 훈련집합의 샘플에 대해 아래 식을 가장 잘 만족하는 W를 찾아내는 작업\n",
    "                    학습이라는 과업: o=taur(Wx)\n",
    "            * 현대 기계 학습에서 퍼셉트론의 중요성\n",
    "                - 딥러닝은 퍼셉트론을 여러 층으로 확장하여 만듦\n",
    "\n",
    "        - 선형결합과 벡터 공간\n",
    "            * 벡터\n",
    "                -공간상의 한 점으로 화살표 끝이 벡터의 좌표에 해당\n",
    "            * 선형결합이 만드는 벡터 공간\n",
    "                - 기저벡터a와b의 선형결합\n",
    "                    c=alphaa+a2b\n",
    "                - 선형 결합으로 만들어지는 공간을 벡터공간이라 부름\n",
    "\n",
    "        - 역행렬\n",
    "            * 역행렬의 원리\n",
    "                - 정사각행렬의 A의 역행렬 A^-1\n",
    "                A^-1 * A=A*A^-1=1\n",
    "            - 예를들어, (2 1)             (2  -0.5)\n",
    "                        6 5  의 역행렬은   -3  1     \n",
    "            * 역행렬의 필요 충분 조건 \n",
    "                - A는 역행렬을 가진다. 즉, 특이행렬이아니다.\n",
    "                - A는 최대계수를 가진다\n",
    "                - A의 모든 행이 선형독립이다.\n",
    "                - A의 모든 열이 선형독립이다.\n",
    "                - A의 행렬식은 0이 아니다.\n",
    "                - ATA는 양의 정부호 대칭 행렬이다.\n",
    "                - A의 고윳값 모두 0이 아니다.\n",
    "            * 역행렬 공식\n",
    "                A^-1=adj A/det A\n",
    "            * 2*2 행렬에 대한 역행렬\n",
    "                [A11 A12]^-1                     [A22  -A12]\n",
    "                 A21 A22      = 1/(A11A22-A12A21) -A21  A11\n",
    "            * 3*3 행렬에 대한 역행렬\n",
    "-> 추가해야함\n",
    "            * 행렬 A의 행렬식 det(A)\n",
    "                det(a b)\n",
    "                    c d    = ad -bc\n",
    "                det(a b c)\n",
    "                    d e f\n",
    "                    g h i     = aei+bfg+cdh-ceg-bdi-afh\n",
    "            * 기하학적 의미\n",
    "                - 2차원에서는 2개의 행 벡터가 이루는 평행사변형의 넓이\n",
    "                - 3차원에서는 3개의 행 벡터가 이루는 평행사각기둥의 부피\n",
    "\n",
    "            - 행렬분해\n",
    "                * 분해란?\n",
    "                    - 정수 3717은 특성이 보이지 않지만, 3*3*7*59로 소인수 분해를 하면 특성이 보이듯이, 행렬도 분해하면 여러모로 유용함\n",
    "                * 고윳값과 고유 벡터\n",
    "                    - 고유 벡터 v와 고윳값 λ\n",
    "                    Av=λv\n",
    "                * 고윳값과 고유 벡터의 기하학적 해석\n",
    "-> 추가해야함\n",
    "                * 고윳값 분해 \n",
    "                    A=QAQ^-1\n",
    "                    - Q는 A의 고유벡터를 열에 배치한 행렬이고 A는 고윳값을 대각선에 배치한 대각행렬\n",
    "                    - EX: (2 1)   (1  1)(3  0)(0.5  0.5)\n",
    "                           1 2  =  1 -1  0  1  0.5  -0.5\n",
    "                    - 고윳값 분해는 정사각행렬에만 적용 가능한데, 기계학습에서는 정사각행령이 아닌 경우의 분해도 필요하므로 고윳값 분해는 한계를 가짐\n",
    "                 * n*m 행렬 A의 특잇값 분해\n",
    "                    A= UΣV^T\n",
    "                    - 왼쪽 특이행렬 U는 AAT의 고유 벡터를 열에 배치한 n*n행렬\n",
    "                    - 오른쪽 특이행렬 V는 ATA의 고유벡터를 열에 배치한 m*m대각행렬\n",
    "-> 추가해야함\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3주차"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 1) 평균 벡터와 공분산 행렬 예제 (3주차 강의자료 p.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.array([[5.1,3.5,1.4,0.2], [4.9,3.0,1.4,0.2], [4.7,3.2,1.3,0.2], [4.6,3.1,1.5,0.2], [5.0,3.6,1.4,0.2], [5.4,3.9,1.7,0.4], [4.6,3.4,1.4,0.3], [5.0,3.4,1.5,0.2]])\n",
    "print(x, '\\n\\n')\n",
    "\n",
    "x_mean = x.mean(axis=0)\n",
    "print(x_mean, '\\n\\n')\n",
    "\n",
    "x_var = np.cov(x.T, ddof=0)\n",
    "print(x_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 2) 자기정보와 엔트로피 예제 (3주차 강의자료 p.19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "prob_yut = np.array([4/16, 6/16, 4/16, 1/16, 1/16])\n",
    "\n",
    "print(prob_yut, '\\n\\n')\n",
    "\n",
    "H_yut = 0;\n",
    "for i in prob_yut:\n",
    "  H_yut += - i*math.log2(i)\n",
    "\n",
    "print(H_yut, '\\n\\n')\n",
    "\n",
    "prob_dice = np.array([1/6, 1/6, 1/6, 1/6, 1/6, 1/6])\n",
    "\n",
    "H_dice = 0;\n",
    "for i in prob_dice:\n",
    "  H_dice += - i*math.log2(i)\n",
    "\n",
    "print(H_dice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4주차 베이시언 결정 이론"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정규분포와 분별함수 베이시언 분류기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "mu = np.array([3, 1])\n",
    "\n",
    "Sigma = np.array([[2, 0],[0, 2]])\n",
    "\n",
    "det_Sigma = np.linalg.det(Sigma)\n",
    "print('Determinant of Sigma=\\n', det_Sigma)\n",
    "\n",
    "inv_Sigma = np.linalg.inv(Sigma)\n",
    "print('Inverse matrix of Sigma=\\n', inv_Sigma)\n",
    "\n",
    "coef1 = -1/2*inv_Sigma\n",
    "coef2 = mu.T @ inv_Sigma\n",
    "coef3 = -1/2*mu.T @ inv_Sigma @ mu - np.log(2*3.14) - 1/2*np.log(det_Sigma)\n",
    "\n",
    "print('Coefficient 1 =\\n', coef1)\n",
    "print('Coefficient 2 =\\n', coef2)\n",
    "print('Coefficient 3 =\\n', coef3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "선형 분별 정규분포에서 베이시언분류기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "omega_1 = np.transpose(np.array([[1,2],[3,1],[5,2],[3,3]]))\n",
    "\n",
    "print(omega_1)\n",
    "\n",
    "omega1_mean = omega_1.mean(axis=1)\n",
    "omega1_var = np.cov(omega_1, ddof=0)\n",
    "\n",
    "omega_2 = np.transpose(np.array([[6,6],[8,5],[10,6],[8,7]]))\n",
    "omega2_mean = omega_2.mean(axis=1)\n",
    "omega2_var = np.cov(omega_2, ddof=0)\n",
    "\n",
    "print('mu_1=', omega1_mean)\n",
    "print('mu_2=', omega2_mean)\n",
    "\n",
    "print('Sigma_1=\\n', omega1_var)\n",
    "print('Sigma_2=\\n', omega2_var)\n",
    "\n",
    "inv_Sigma = np.linalg.inv(omega1_var)\n",
    "\n",
    "coef_1 = inv_Sigma @ (omega1_mean - omega2_mean)\n",
    "coef_2 = - 1/2*omega1_mean.T @ inv_Sigma @ omega1_mean + 1/2*omega2_mean.T @ inv_Sigma @ omega2_mean\n",
    "\n",
    "print('coef_1=', coef_1)\n",
    "print('coef_2=', coef_2)\n",
    "\n",
    "test_x = np.transpose(np.array([8,2]))\n",
    "\n",
    "prob_w1 = 0.5\n",
    "prob_w2 = 0.5\n",
    "\n",
    "g12 = coef_1 @ test_x + coef_2 + np.log(prob_w1) - np.log(prob_w2)\n",
    "\n",
    "print('g12(test_x) = ',g12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결정직선과함꼐 출력 \n",
    "# Let's correct the approach for calculating and plotting the decision boundary based on given coefficients and data.\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Given data for omega_1 and omega_2\n",
    "omega_1 = np.array([[1, 2], [3, 1], [5, 2], [3, 3]]).T  # Transpose to match provided code's structure\n",
    "omega_2 = np.array([[6, 6], [8, 5], [10, 6], [8, 7]]).T\n",
    "\n",
    "# Calculated means\n",
    "omega1_mean = np.mean(omega_1, axis=1)\n",
    "omega2_mean = np.mean(omega_2, axis=1)\n",
    "\n",
    "# Combined covariance and its inverse\n",
    "omega1_var = np.cov(omega_1)\n",
    "omega2_var = np.cov(omega_2)\n",
    "inv_Sigma = np.linalg.inv((omega1_var + omega2_var) / 2)\n",
    "\n",
    "# Coefficients for linear decision boundary\n",
    "coef_1 = inv_Sigma @ (omega1_mean - omega2_mean)\n",
    "coef_2 = -0.5 * (omega1_mean.T @ inv_Sigma @ omega1_mean) + 0.5 * (omega2_mean.T @ inv_Sigma @ omega2_mean) + np.log(0.5 / 0.5)\n",
    "\n",
    "# Plotting data points\n",
    "plt.scatter(omega_1[0], omega_1[1], color='red', label='Class 1')\n",
    "plt.scatter(omega_2[0], omega_2[1], color='blue', label='Class 2')\n",
    "\n",
    "# Generating points for decision boundary\n",
    "x = np.linspace(0, 10, 100)\n",
    "y = (-coef_1[0] * x - coef_2) / coef_1[1]\n",
    "plt.plot(x, y, label='Decision Boundary', color='green')\n",
    "\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend()\n",
    "plt.title('Data Points and Decision Boundary')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2차 분별 정규분포에서 베이시언 분류기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "omega_1 = np.transpose(np.array([[1,2],[3,1],[5,2],[3,3]]))#, [4,2], [2,4], [1,1], [2,2], [4,4]]))\n",
    "y_omega1 = np.array([0, 0, 0, 0])#, 0, 0, 0, 0, 0])\n",
    "omega1_mean = omega_1.mean(axis=1)\n",
    "omega1_var = np.cov(omega_1, ddof=0)\n",
    "\n",
    "inv_Sigma1 = np.linalg.inv(omega1_var)\n",
    "\n",
    "coef1_for_g1 = - 1/2 * inv_Sigma1\n",
    "coef2_for_g1 = omega1_mean.T @ inv_Sigma1\n",
    "coef3_for_g1 = - 1/2 * omega1_mean.T @ inv_Sigma1 @ omega1_mean - 1/2 * np.log(np.linalg.det(omega1_var))\n",
    "\n",
    "print('coef_1 = \\n', coef1_for_g1)\n",
    "print('coef_2 = \\n', coef2_for_g1)\n",
    "print('coef_3 = \\n', coef3_for_g1)\n",
    "\n",
    "\n",
    "omega_2 = np.transpose(np.array([[7,6],[8,4],[9,6],[8,8]]))\n",
    "y_omega2 = np.array([1, 1, 1, 1])\n",
    "omega2_mean = omega_2.mean(axis=1)\n",
    "omega2_var = np.cov(omega_2, ddof=0)\n",
    "\n",
    "x_train = (np.concatenate([omega_1.T, omega_2.T]))\n",
    "y_train = np.concatenate([y_omega1, y_omega2])\n",
    "\n",
    "plt.plot(x_train[y_train==0, 0], x_train[y_train==0, 1], 'ro', x_train[y_train==1, 0], x_train[y_train==1, 1], 'bs')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print('mu_1=', omega1_mean)\n",
    "print('mu_2=', omega2_mean)\n",
    "\n",
    "print('Sigma_1=\\n', omega1_var)\n",
    "print('Sigma_2=\\n', omega2_var)\n",
    "print('\\n')\n",
    "\n",
    "inv_Sigma2 = np.linalg.inv(omega2_var)\n",
    "\n",
    "coef1_for_g2 = - 1/2 * inv_Sigma2\n",
    "coef2_for_g2 = omega2_mean.T@inv_Sigma2\n",
    "coef3_for_g2 = - 1/2 * omega2_mean.T @ inv_Sigma2 @ omega2_mean - 1/2 * np.log(np.linalg.det(omega2_var))\n",
    "\n",
    "coef1_for_g12 = coef1_for_g1 - coef1_for_g2\n",
    "coef2_for_g12 = coef2_for_g1 - coef2_for_g2\n",
    "coef3_for_g12 = coef3_for_g1 - coef3_for_g2\n",
    "\n",
    "print('coef_1 = \\n', coef1_for_g12)\n",
    "print('coef_2 =', coef2_for_g12)\n",
    "print('coef_3 =', coef3_for_g12)\n",
    "\n",
    "coef3_for_g12_08_02 = coef3_for_g12 + np.log(0.8)-np.log(0.2)\n",
    "coef3_for_g12_05_05 = coef3_for_g12 + np.log(0.5)-np.log(0.5)\n",
    "coef3_for_g12_02_08 = coef3_for_g12 + np.log(0.2)-np.log(0.8)\n",
    "\n",
    "print('coef_3_08_02 =', 4*coef3_for_g12_08_02)\n",
    "print('coef_3_05_05 =', 4*coef3_for_g12_05_05)\n",
    "print('coef_3_02_08 =', 4*coef3_for_g12_02_08)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결정경계 코드\n",
    "# 수정된 전체 코드 실행\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# 데이터 생성\n",
    "omega_1 = np.array([[1, 2], [3, 1], [5, 2], [3, 3]])  # 클래스 1\n",
    "omega_2 = np.array([[7, 6], [8, 4], [9, 6], [8, 8]])  # 클래스 2\n",
    "\n",
    "# 클래스별 평균 벡터 계산\n",
    "omega1_mean = np.mean(omega_1, axis=0)\n",
    "omega2_mean = np.mean(omega_2, axis=0)\n",
    "\n",
    "# 클래스별 공분산 행렬 계산 및 역행렬 계산\n",
    "omega1_cov = np.cov(omega_1.T)\n",
    "inv_omega1_cov = np.linalg.inv(omega1_cov)\n",
    "\n",
    "omega2_cov = np.cov(omega_2.T)\n",
    "inv_omega2_cov = np.linalg.inv(omega2_cov)\n",
    "\n",
    "# 계수 계산\n",
    "coef1_for_g1 = -0.5 * inv_omega1_cov\n",
    "coef2_for_g1 = inv_omega1_cov @ omega1_mean\n",
    "coef3_for_g1 = -0.5 * (omega1_mean.T @ inv_omega1_cov @ omega1_mean + np.log(np.linalg.det(omega1_cov)))\n",
    "\n",
    "coef1_for_g2 = -0.5 * inv_omega2_cov\n",
    "coef2_for_g2 = inv_omega2_cov @ omega2_mean\n",
    "coef3_for_g2 = -0.5 * (omega2_mean.T @ inv_omega2_cov @ omega2_mean + np.log(np.linalg.det(omega2_cov)))\n",
    "\n",
    "# 결정 경계 계수\n",
    "coef1_for_g12 = coef1_for_g1 - coef1_for_g2\n",
    "coef2_for_g12 = coef2_for_g1 - coef2_for_g2\n",
    "coef3_for_g12 = coef3_for_g1 - coef3_for_g2\n",
    "\n",
    "# 결정 경계 시각화\n",
    "x_min, x_max = 0, 10\n",
    "y_min, y_max = 0, 10\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))\n",
    "\n",
    "# 결정 함수\n",
    "def decision_function(x, y, coef1, coef2, coef3):\n",
    "    xy = np.array([x, y])\n",
    "    return np.dot(xy.T, np.dot(coef1, xy)) + np.dot(coef2, xy) + coef3\n",
    "\n",
    "# 결정 경계 생성\n",
    "Z = np.array([decision_function(x, y, coef1_for_g12, coef2_for_g12, coef3_for_g12) for x, y in zip(np.ravel(xx), np.ravel(yy))])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# 데이터 및 결정 경계 시각화\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(omega_1[:, 0], omega_1[:, 1], 'ro', label='Class 1')\n",
    "plt.plot(omega_2[:, 0], omega_2[:, 1], 'bo', label='Class 2')\n",
    "plt.contour(xx, yy, Z, levels=[0], colors='green')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Decision Boundary between Class 1 and Class 2')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "최소 거리 분류기 정규분포에서 베이시언 분류기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "omega_1 = np.transpose(np.array([[1,2],[3,1],[5,2],[3,3]]))\n",
    "omega1_mean = omega_1.mean(axis=1)\n",
    "\n",
    "omega_2 = np.transpose(np.array([[6,6],[8,5],[10,6],[8,7]]))\n",
    "omega2_mean = omega_2.mean(axis=1)\n",
    "\n",
    "inv_Sigma = np.linalg.inv(omega1_var)\n",
    "\n",
    "print(inv_Sigma)\n",
    "\n",
    "x = np.transpose(np.array([2, 8]))\n",
    "\n",
    "Mahala_dist1 = math.sqrt( (x - omega1_mean).T @ inv_Sigma @ (x - omega1_mean) )\n",
    "Mahala_dist2 = math.sqrt( (x - omega2_mean).T @ inv_Sigma @ (x - omega2_mean) )\n",
    "\n",
    "print(Mahala_dist1)\n",
    "print(Mahala_dist2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마할노비스로 결정직선 그래프로 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 데이터 생성\n",
    "omega_1 = np.transpose(np.array([[180, 78], [167, 72], [170, 66], [178, 80]])) # 남자\n",
    "omega_2 = np.transpose(np.array([[160, 48], [155, 45], [167, 55], [158, 60]])) # 여자\n",
    "X = np.hstack([omega_1, omega_2])\n",
    "y = np.array([0] * len(omega_1) + [1] * len(omega_2))\n",
    "\n",
    "# 테스트 샘플\n",
    "x = np.transpose(np.array([165, 65]))\n",
    "\n",
    "# 클래스별 평균 벡터 계산\n",
    "mean_omega_1 = np.mean(omega_1, axis=1)\n",
    "mean_omega_2 = np.mean(omega_2, axis=1)\n",
    "\n",
    "# 클래스별 공분산 행렬 계산\n",
    "cov_omega_1 = np.cov(omega_1)\n",
    "cov_omega_2 = np.cov(omega_2)\n",
    "\n",
    "# 역 공분산 행렬 계산\n",
    "inv_cov_omega_1 = np.linalg.inv(cov_omega_1)\n",
    "inv_cov_omega_2 = np.linalg.inv(cov_omega_2)\n",
    "\n",
    "# 평균 벡터 그리기\n",
    "plt.plot(mean_omega_1[0], mean_omega_1[1], 'ro', markersize=10, label='Man Class Mean')\n",
    "plt.plot(mean_omega_2[0], mean_omega_2[1], 'bs', markersize=10, label='Woman Class Mean')\n",
    "\n",
    "# 테스트 샘플 출력\n",
    "plt.scatter(x[0], x[1], c='g', label='Test Sample')\n",
    "\n",
    "# 훈련 데이터 출력\n",
    "plt.scatter(omega_1[0], omega_1[1], c='r', label='Man')\n",
    "plt.scatter(omega_2[0], omega_2[1], c='b', label='Woman')\n",
    "\n",
    "x_min, x_max = X[0].min() - 1, X[0].max() + 1\n",
    "y_min, y_max = X[1].min() - 1, X[1].max() + 1\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))\n",
    "\n",
    "Z = np.zeros(xx.shape)\n",
    "\n",
    "# 두 평균 벡터를 이어주기\n",
    "plt.plot([mean_omega_1[0], mean_omega_2[0]], [mean_omega_1[1], mean_omega_2[1]], color='black', linestyle='--')\n",
    "\n",
    "# Mahalanobis 거리 계산 함수\n",
    "def mahalanobis_distance(x, mean, inv_covariance):\n",
    "    x_minus_mean = x - mean\n",
    "    mahala_dist = np.sqrt(np.dot(np.dot(x_minus_mean.T, inv_covariance), x_minus_mean))\n",
    "    return mahala_dist\n",
    "\n",
    "# 남자와 여자의 Mahalanobis 거리 계산 및 결정 경계 그리기\n",
    "for i in range(len(xx)):\n",
    "    for j in range(len(yy)):\n",
    "        point = np.array([xx[i, j], yy[i, j]])\n",
    "        dist_omega_1 = mahalanobis_distance(point, mean_omega_1, inv_cov_omega_1)\n",
    "        dist_omega_2 = mahalanobis_distance(point, mean_omega_2, inv_cov_omega_2)\n",
    "        Z[i, j] = dist_omega_1 - dist_omega_2\n",
    "\n",
    "plt.contour(xx, yy, Z, levels=[0], colors='green')\n",
    "\n",
    "plt.xlabel('Height')\n",
    "plt.ylabel('Weight')\n",
    "plt.title('Decision Boundary with Mahalanobis Distance')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# 남자 클래스와 여자 클래스의 Mahalanobis 거리 계산\n",
    "Mahala_dist1 = mahalanobis_distance(x, mean_omega_1, inv_cov_omega_1)\n",
    "Mahala_dist2 = mahalanobis_distance(x, mean_omega_2, inv_cov_omega_2)\n",
    "\n",
    "# 남자인지 여자인지 결정\n",
    "if Mahala_dist1 < Mahala_dist2:\n",
    "    print(\"테스트 샘플은 남자에 속합니다.\")\n",
    "else:\n",
    "    print(\"테스트 샘플은 여자에 속합니다.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
