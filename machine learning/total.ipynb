{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1주차 기계학습이란\n",
    "\n",
    "\n",
    "기계학습 소개\n",
    "    인공지능, 머신러닝, 딥러닝의 개념\n",
    "        (1) 인공지능: 사고나 학습등 인간이 가진 지적 능력을 컴퓨터를 통해 구현하는 기술\n",
    "        (2) 머신러닝: 컴퓨터가 스스로 학습하여 인공지능의 성능을 향상시키는 기술 방법\n",
    "        (3) 딥러닝: 인간이 뉴러과 비슷한 인공신경망 방식으로 정보를 처리\n",
    "\n",
    "    기계학습의 정의와 중요성\n",
    "        - 학습이란? <표준국어대사전>: \"겸헝의 결과로 나타나는, 비교적 지속적인 행동의 변화나 그 잠재력의 변화, 또는 지식을 습득하는 과정\n",
    "        - 기계학습이란?: 인공지능 초창기 사무엘의 정리\n",
    "            \" 컴퓨터가 경험을 통해 학습할 수 있도록 프로그래밍할 수 있다면, 세세하게 프로그래밍해야 하는 번거로움에 벗어날 수 있다.\n",
    "        - 기계 학습이란? - 현대적 정의\n",
    "            \"어떤 컴퓨터 프로그램이 T라는 작업을 수행한다. 이 프로그램의 성능을 P라는 척도로 평가했을 때 경험E를 통해 성능이 개선된다면 이 프로그램은 학습을 한다고 말할 수 있다.\n",
    "            \" 사례 데이터, 즉 과거 경험을 이용하여 성능 기준을 최적화하도록 프로그래밍하는 작업\n",
    "            \" 성능을 개선하거나 정확하게 예측하기 위해 경험을 이용하는 계산학 방법들 \n",
    "        - 자식기반 방식에서 기계 학습으로의 대전환 \n",
    "            - 인공지능의 탄생\n",
    "                - 컴퓨터의 뛰어난 능력\n",
    "                    - 사람이 어려워하는 일을 아주 쉽게함\n",
    "                    - 80932.46789076*0.39001324와 같은 곱셈을 고속으로 수행\n",
    "                    - 복잡한 함수의 미분과 적분 척척\n",
    "                - 컴퓨터에 대한 기대감(컴퓨타의 능력 과신)\n",
    "                    - 사람이 쉽게하는 일, 예를들어 고양이/개 구별하는 일도 잘 하지 않을까\n",
    "                    - 1950년대에 인공지능이라는 분야 등장\n",
    "            \n",
    "            - 초창기는 지식기반 방식이 주류\n",
    "                - ex: \"구멍이 2개이고 중간 부분이 홀쭉하며, 맨 위와 아래가 둥근 모양이라면 8이다.\"\n",
    "            - 큰 꺠달음\n",
    "                - 지식기반의 한계\n",
    "                - 단추를 \"가운데 구멍이 몇 개 있는 물체\"라고 규정하면 많은 오류 발생\"\n",
    "                - 사람은 변화가 심한 장면을 아주 쉽게 인식하지만, 왜 그렇게 인식하는지 서술하지는 못함\n",
    "            - 인공지능의 주도권 전환\n",
    "                - 지식기반-> 기계학습\n",
    "                - 기계학습: 데이터 중심 접근방식\n",
    "\n",
    "        기계학습 개념\n",
    "            - 간단한 기계학습 예제\n",
    "                - 가로축은 시간, 세로축은 이동체의 위치\n",
    "                - 관측한 4개의 점이 데이터\n",
    "            - 예측 문제\n",
    "                - 임의의 시간이 주어지면 이떄 이동체의 위치는?\n",
    "                - 회귀문제와 분류 문제로 나뉨\n",
    "                    - 회귀는 목표치가 실수, 분류는 부류값\n",
    "            - 훈련집합\n",
    "                - 가로축은 특징, 세로축은 목표치\n",
    "                - 관측한 4개의 점이 훈련집합을 구서함\n",
    "                훈련집합 x={x1,x2,..., Xn}, Y={y1,y2,..,Yn}\n",
    "            - 데이터를 어떻게 모델링할 것인가\n",
    "                -눈대중으로 보면 직석을 이루므로 직선을 선택하자-> 모델로 직선을 선택한 셈\n",
    "            - 직선 모델의 수식\n",
    "                - 2개의 매개변수w와 b\n",
    "                y=wx+b\n",
    "            - 기계학습은\n",
    "                - 가장 정확하게 예측할 수 있는. 즉 최적의 매개변수를 찾는 작업\n",
    "                - 처음에는 최적값을 모르므로 임의의 값에서 시작하고. 점점 성능을 개선하여 최적에 도달\n",
    "            - 학습에 마치면\n",
    "                - 예측에 사용\n",
    "                - 10.0순간의 이동체 위치를 알고자하면, f3(10.0)=0.5*10.0+2.0=7.0이라고 예측함\n",
    "            - 기계학습의 궁극적인 목표\n",
    "                - 훈련집합에 없는 새로운 샘플에 대한 오류를 최소화(새로운 샘플집합: 테스트 집합)\n",
    "                - 테스트 집합에 대한 높은 성능을 일반화 능력이라고 부름\n",
    "            - 사람의 학습과 기계학습\n",
    "-> 추가해야함                \n",
    "    특징 공간과 데이터에 대한 이해\n",
    "        - 1차원과 2차원 특징 공간\n",
    "            - 1차원 특징공간\n",
    "            - 2차원 특징공간\n",
    "                -특징 벡터 표기\n",
    "                    - x=(x1,x2)T\n",
    "                - EX\n",
    "                    - x=(몸무게 키)T, y=장타율\n",
    "                    - x=(체온, 두통)T, y=감기여부\n",
    "\n",
    "        - 다차원 특징 공간\n",
    "            - 다차원 특징공간예제\n",
    "            - d-차원데이터\n",
    "                - 특징벡터 표기 x=(x1,x2,...,xd)T\n",
    "            - d- 차원데이터를 위한 학습 모델\n",
    "                - 직선 모델을 사용하는 경우 매개변수 수= d+1\n",
    "                    y=w1x1+w2x2+...+wdxd+b\n",
    "                - 2차 곡선모델을 사용하면 매개변수 수가 크게 증가\n",
    "                    - 매개변수 수= d^2+d+1\n",
    "                    - ex: lris 데이터: d=4이므로 21개의 매개변수\n",
    "                    - ex: MNIST 데이터: D=784이므로 615,441개의 매개변수\n",
    "                    y=w1x^2+ w2x2/2+...wdxd^2+ wd+1x1x2+...wd2xd-1xd+wd^2+1x1+...+Wd^2+dxd+b\n",
    "        - 데이터에 대한 이해\n",
    "            -> 이해\n",
    "                - 과학 기술의 발전과정\n",
    "                    데이터 수집-> 모델 정립-> 예측\n",
    "                - 기계 학습\n",
    "                    -기계 학습이 푸는 문제는 훨씬 복잡함\n",
    "                        ex- '8' 숫자 패턴과 '단추' 패턴의 다양한 변화 양상\n",
    "                    - 단순한 수학 공식으로 표현 불가능\n",
    "                    - 자동을 모델을 찾아내는 과정이 필수\n",
    "            - 데이터 생성과정\n",
    "                - 데이터 생성 과정을 완전히 아는 인위적 상황의 예제\n",
    "                    ex- 두 개 주사위를 던져 나온 눈의 합을 x라 할 때, y=(x-7)^2+1점을 받는 게임\n",
    "                    - 이런 상황을 '데이터 생성 과정을 완전히 알고 있다'고 말함\n",
    "                        - x를 알면 정확히 y를 예측할 수 있음\n",
    "                            - 실제 주사위를 던져 x={3,10,8.5}를 얻었다면,y={17,10,2.5}\n",
    "                        -x의 발생 확률 P(x)를 정확히 알 수 있음\n",
    "                        - P(x)를 알고 있으므로, 새로운 데이터 생성 가능\n",
    "                - 실제 기계 학습문제\n",
    "                    - 데이터 생성 광정을 알 수 없음\n",
    "                    - 단지 주어진 훈련집합 X,Y로 예측 모델 또는 생성 모델을 근사 추정할 수 있을 뿐 \n",
    "\n",
    "            - 데이터 베이스의 중요성\n",
    "                - 데이터베이스의 품질\n",
    "                    - 주어진 응용에 맞느 충분히 다양한 데이터를 충분한 양만큼 수집-> 추정 정확도 높아짐\n",
    "                        - 예: 정면 얼굴만 가진 데이터베이스로 학습하고 나면, 기운 얼굴은 매운 낮은 성능\n",
    "                        -> 주어진 응용 환경을 자세히 살핀 다음 그에 맞는 데이터 베이스 확보는 아주 중요함\n",
    "                - 아주 많은 공개 데이터베이스\n",
    "                    - 기계 학습의 초파리로 여겨지는 3가지 데이터베이스:Iris, MNIST, ImageNet\n",
    "                    - 위키피디아에서 'list of datasets for machine learning research'로 검색\n",
    "                    - UCI Repository(현재 기준으로 622개 데이터베이스 제공)\n",
    "\n",
    "            - 데이터베이스 크기와 기계학습 성능\n",
    "                - 데이터베이스의 왜소한 크기\n",
    "                    -ex: MNIST: 28*28 흑백 비트맵이라면 서로 다른 총 샘플 수는 2^784가지이지만, MNIST는 고작 6만 개 샘플\n",
    "                - 왜소한 데이터 베이스로 어떻게 높은 성능을 달성하는가?\n",
    "                    - 방대한 공간에서 실제 데이터가 발생하는 곳은 매우 작은 부분 공간임\n",
    "                    - 매니 폴드 가정\n",
    "                        - 2222 와같이 일정한 규칙에 따라 매끄럽게 변화\n",
    "\n",
    "            - 데이터 가시화 \n",
    "                - 4차원 이상의 초공간은 한꺼번에 가시화 불가능\n",
    "                - 여러 가지 가시화 기법\n",
    "                    - 2개씩 조합하여 여러 개의 그래프 그림\n",
    "\n",
    "    기계학습의 예와 모델 선택\n",
    "        - 과소적합과 과잉 적합\n",
    "            - 아래 그름의 1차 모델은 과소적합\n",
    "                - 모델의 '용량이 작아' 오차가 클 수 밖에 없는 현상\n",
    "            - 비선형 모델을 사용하는 대안\n",
    "                - 2차, 3차, 4차, 12차는 다항식 곡선을 선택한 예\n",
    "                - 1차(선형)에 비해 오차가 크게 감소함\n",
    "\n",
    "            - 과잉 적합\n",
    "                - 12차 다항식 곡선을 채택한다면 훈련집합에 대해 거의 완벽하게 근사화함\n",
    "                - 하지만 '새로운' 데이터를 예측해야 하지만 빨간점을 예측 \n",
    "                - 이유는 '용량이 크기'때문. 학습과정에서 잡음까지 수용-> 과잉적합 현상\n",
    "                - 적절한 용얄의 모델을 선택하는 모델 선택 작업이 필요함\n",
    "\n",
    "        - 바이어스와 분산\n",
    "            - 1차~12차 다항식 모델의 비교관찰\n",
    "                - 1~2차는 훈련집합과 테스트집합 모두 낮은 성능\n",
    "                - 12차는 훈련집합에 높은 성능을 보이나 테스트집합에는 낮은 성능-> 낮은 일반화 능력\n",
    "                - 3~4차는 훈련집합에 대해 12차보다는 낮겠지만 테스트 집합에는 높은 성능\n",
    "                    -> 높은 일반화 능력\n",
    "            \n",
    "            - 훈련집합을 여러 번 수집하여 1차~12차에 적용하는 실험\n",
    "                - 2차는 매번 큰 오차-> 바이어스가 큼. 하지만 비슷한 모델을 얻음 -> 낮은 분산\n",
    "                - 12차는 매번 작은 오차-> 바이어스가 작음. 하지만 크게 다른 모델을 얻음-> 높은 분산\n",
    "                - 일반적으로 용량이 적은 모델은 바이어스는 크고 분산은 작음. 복잡한 모델은 바이어스는 작고 분산은 큼\n",
    "                - 바이어스와 분산은 트레이드오프 관계\n",
    "            \n",
    "            - 기계학습의 목표\n",
    "                - 낮은 바이어스와 낮은 분산을 가진 예측기 제작이 목표, 즉 왼쪽 아래 상황\n",
    "                    - 하지만 바이어스와 분산은 트레이드오프관계\n",
    "                    - 따라서 바이어스 희생을 최소로 유지하며 분산을 최대로 낮추는 전략이 필요\n",
    "                    \n",
    "        - 모델 선택의 한계와 현실적인 해결책 \n",
    "            - 알고리즘에서 모델 집합 옴\n",
    "                - 서로 다른 차수의 다항식이 옴(1차,2차,3차,...,12차)\n",
    "                - 현실에서는 아주 다양\n",
    "                    -Support Vector Machine(SVM), 트리분류기, 신경망, 강화 학습 등이 선택 대상\n",
    "            - 현실에서는 경험으로 큰 틀 먼저 선택\n",
    "                -모델 선택 알고리즘으로 세부 모델 선택하는 전략 사용\n",
    "            - 이런 경험적인 접근 방법에 대한 'DEEP LEARNING'책의 비유\n",
    "                \"어느 정도 우리가 하는 일은 항상 둥근 홈(우리가 선택한 모델)에 네모 막대기(데이터 생성 과정)을 끼워 넣는 것이라고 말할 수 있다\"\n",
    "            \n",
    "            현대 기계학습의 전략\n",
    "                - 용량이 충분히 큰 모델을 선택한 후, 선택한 모델이 정상을 벗어나지 않도록 여러가제 규제 기번을 적용함\n",
    "                - EX: 12차 다항식을 선택한 후 적절히 규제 적용\n",
    "\n",
    "    규제의 개념\n",
    "        - 데이터 확대\n",
    "            - 데이터를 더 많이 수집하면 일반화 능력이 향상됨\n",
    "            - 데이터 수집은 많은 비용이 듬\n",
    "                - 그라운드 트루스를 사람이 일일이 레이블링해야 함\n",
    "            - 인위적으로 데이터 확대\n",
    "                - 훈련집합에 있는 샘프을 변형함\n",
    "                - 약간 회전 또는 와핑(부류 소속이 변하지 않게 주의)\n",
    "\n",
    "        - 가중치 감쇠\n",
    "            - 가중치를 작게 조잘하는 기법\n",
    "                - 아래 12차 곡선은 가중치가 매우 큼\n",
    "                    y=1005.7x^12-27774.4x^11+...-22852612.5x^1-12.8\n",
    "            - 가중치 감쇠는 개선된 목적함수를 이용하여 가중치를 작게 조절하는 규제 기법\n",
    "                - 식(1.11)의 두 번째 항으로서 가중치 크기를 작게 유지해줌\n",
    "-> 추가해야함\n",
    "\n",
    "기계학습 유형 \n",
    "    - 지도 방식에 따른 유형\n",
    "        - 지도 학습(Supervised Learning)\n",
    "            - 정답이 주어진 상태에서 학습하는 알고리즘\n",
    "            - 훈련데이터로부터 하나의 함수를 유추\n",
    "            - 지도학습 종류\n",
    "                - 회귀분석(Regression)/예측: 연속적인 값을 출력하는 것\n",
    "                - 분류(Classifiaciton): 주어진 입력 벡터가 어떤 종류의 값인지 표시하는 것\n",
    "        - 비지도 학습(Unsupervised Learning)\n",
    "            - 정답이 주어지지 않은 상태에서 데이터의 특정을 학습하는 알고리즘\n",
    "            - 데이터가 어떻게 구성되었는지를 알아내는 문제를 해결\n",
    "            - 지도학습혹은 강화 학습과는 달리 입력값에 대한 목표치(분류된 값)기 없음\n",
    "            - 비지도학습 종류: 군집화, 연관규칙\n",
    "                -데이터의 주요 특징을 요약하고 설명\n",
    "        - 강화학습(Reinforcement Learning)\n",
    "            - 보상 혹은 벌칙과 함께 여러 번의 시행착오를 거쳐 스스로 학습하는 방법\n",
    "                - 분류할 수 있는 데이터 혹은 정답이 존재하지 않음\n",
    "                - 행동에 보상 혹은 벌칙을 받으며 학습\n",
    "            - 보상을 최대한 많이 얻도록 하는 행동을 유도하도록 학습을 진행\n",
    "            - 예제: 아이가 일어서고 걷는 방법, 구글의 알파고 \n",
    "\n",
    "\n",
    "기계학습과 수학\n",
    "    선형대수, 확률과 통계, 최적화의 중요성\n",
    "    기계학습에서 수학이 사용되는 방식\n",
    "\n",
    "지도 학습(Supervised Learning)\n",
    "    통계적 분류, 신경망 및 퍼셉트론\n",
    "    SVM (Support Vector Machine)\n",
    "    질적 분류 및 결정 트리\n",
    "\n",
    "비지도 학습(Unsupervised Learning)\n",
    "    군집화(Clustering)\n",
    "\n",
    "준지도 학습 & 강화 학습(Semi-supervised & Reinforcement Learning)\n",
    "    준지도 학습과 강화 학습의 기본 개념\n",
    "\n",
    "모델 선택\n",
    "    과소적합과 과대적합\n",
    "    바이어스와 분산\n",
    "    모델 선택의 한계와 현실적인 해결책\n",
    "\n",
    "규제    \n",
    "    데이터 확대\n",
    "    가중치 감쇠\n",
    "\n",
    "기계학습 유형\n",
    "    지도 방식에 따른 학습 유형: 지도학습, 비지도학습, 강화학습\n",
    "    회귀, 분류\n",
    "    클러스터링, 차원 축소"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
